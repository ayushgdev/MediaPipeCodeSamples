<!DOCTYPE html>
<html>
  <head>
    <title>Face Detection using secondary webcam devices</title>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/camera_utils/camera_utils.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/control_utils/control_utils.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/drawing_utils/drawing_utils.js" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/face_detection/face_detection.js"
      crossorigin="anonymous"></script>
  </head>
  <body>

    <video controls id="video-tag" autoplay hidden>
      <source src="" id="source-tag" />
    </video>
    <canvas class="output_canvas"></canvas>
    <div class="d-flex">
      <button id="btn720">Switch to 720p</button>
      <button id="btn480">Switch to 480p</button>
    </div>

    <script type="module">
      const videoElement = document.getElementById('video-tag')
      const source = document.getElementById('source-tag')
      const btn720 = document.getElementById('btn720')
      const btn480 = document.getElementById('btn480')
      btn480.addEventListener('click',()=> {
        faceDetection.reset()
        queryWithContstraints({
        video: {
            width: 640, height: 480
        }, audio: true
        })
      })
      btn720.addEventListener('click',()=> {
        faceDetection.reset()
        queryWithContstraints({
        video: {
            width: 1280, height: 720
        }, audio: true
        })
      })
      const canvasElement = document.getElementsByClassName('output_canvas')[0];
      const canvasCtx = canvasElement.getContext('2d');

      /**
       * Function to execute once the results from the MediaPipe framework are received
       */
      function onResults(results) {
        // Draw the overlays.
        canvasCtx.save();
        canvasCtx.clearRect(0, 0, canvasElement.width, canvasElement.height);
        if (results.detections.length > 0) {
          drawRectangle(
            canvasCtx, results.detections[0].boundingBox,
            { color: 'white', lineWidth: 2, fillColor: '#00000000' });
          drawLandmarks(canvasCtx, results.detections[0].landmarks, {
            color: 'lightblue',
            radius: 1,
          });
        }
        canvasCtx.restore();
      }

      async function queryWithContstraints(constraints){
        try{
            let localVideoStream = await navigator.mediaDevices.getUserMedia(constraints)
            faceDetection.reset();
            var video = document.querySelector('video');
            video.srcObject = localVideoStream
            if(constraints['video']['width']){
              canvasElement.width = constraints['video']['width']
              canvasElement.height = constraints['video']['height']
            }
            // Note: onloadedmetadata doesn't fire in Chrome when using it with getUserMedia.
            // See crbug.com/110938.
            video.onloadedmetadata = function (e) {
                // Ready to go. Do some stuff.
                const frameProcessing = (now, metadata) => {
                  // Uncomment the following line to test the webcam canvas output without mediapipe processing
                  // canvasCtx.drawImage(videoElement, 0, 0, canvasElement.width, canvasElement.height);

                  // call the MediaPipe Face Detection
                  processFrame()
                  // Re-register the callback to perform on next frame
                  videoElement.requestVideoFrameCallback(frameProcessing)
                }
                // register the callback to perform on next frame
                videoElement.requestVideoFrameCallback(frameProcessing)
            };
        } catch(error){
            console.error('Rejected!', error);
        }
      }
      /**
       * Function to enumerate over all available audio/video devices and get particular 
       * device media stream. Once the stream is available, send it to <video> tag so that 
       * video.requestVideoFrameCallback can be used to process each frame of the video.
       * So the <video> tag works as a proxy to get each frame
       */
      async function onload() {
        queryWithContstraints({
          video: true,audio: true
        })
      }
      /**
       * Function to do MediaPipe framework and perform framedetection
       * This function internally calls the function registered via faceDetection.onResults()
       */
      async function processFrame() {
        // Send the videoElement to the MediaPipe
        await faceDetection.send({ image: videoElement });
      }

      // Create MediaPipe facedetection object
      const faceDetection = new FaceDetection({
        locateFile: (file) => {
          return `https://cdn.jsdelivr.net/npm/@mediapipe/face_detection@0.4/${file}`;
        }
      });
      // Set face detection properties
      faceDetection.setOptions({
        model: 'full',
        minDetectionConfidence: 0.5
      });
      // Set callback to perform once MediaPipe has performed its operations and 
      // results are provided back
      faceDetection.onResults(onResults)
      onload()
    </script>
  </body>
</html>