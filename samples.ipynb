{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "objc[54943]: Class CaptureDelegate is implemented in both /opt/homebrew/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x14ed0e538) and /opt/homebrew/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_videoio.3.4.16.dylib (0x13ddc4860). One of the two will be used. Which one is undefined.\n",
      "objc[54943]: Class CVWindow is implemented in both /opt/homebrew/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x14ed0e588) and /opt/homebrew/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x10c460a68). One of the two will be used. Which one is undefined.\n",
      "objc[54943]: Class CVView is implemented in both /opt/homebrew/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x14ed0e5b0) and /opt/homebrew/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x10c460a90). One of the two will be used. Which one is undefined.\n",
      "objc[54943]: Class CVSlider is implemented in both /opt/homebrew/lib/python3.10/site-packages/cv2/cv2.abi3.so (0x14ed0e5d8) and /opt/homebrew/lib/python3.10/site-packages/mediapipe/.dylibs/libopencv_highgui.3.4.16.dylib (0x10c460ab8). One of the two will be used. Which one is undefined.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video FPS:  0.0\n",
      "Video Frame count: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenCV: Couldn't read video stream from file \"path/to/your/video\"\n",
      "[ERROR:0@0.350] global /Users/xperience/actions-runner/_work/opencv-python/opencv-python/opencv/modules/videoio/src/cap.cpp (166) open VIDEOIO(CV_IMAGES): raised OpenCV exception:\n",
      "\n",
      "OpenCV(4.6.0) /Users/xperience/actions-runner/_work/opencv-python/opencv-python/opencv/modules/videoio/src/cap_images.cpp:253: error: (-5:Bad argument) CAP_IMAGES: can't find starting number (in the name of file): path/to/your/video in function 'icvExtractPattern'\n",
      "\n",
      "\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n"
     ]
    }
   ],
   "source": [
    "## Holistic Solution using Video in Python\n",
    "\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "\n",
    "file = 'path/to/your/video'\n",
    "video = cv2.VideoCapture(file)\n",
    "output = cv2.VideoWriter('output_video.mp4', cv2.VideoWriter_fourcc(*'mp4v'), video.get(cv2.CAP_PROP_FPS), (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "print(\"Video FPS: \", video.get(cv2.CAP_PROP_FPS))\n",
    "print(\"Video Frame count:\", video.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "with mp_holistic.Holistic(\n",
    "    static_image_mode=False,\n",
    "    model_complexity=2,\n",
    "    enable_segmentation=False,\n",
    "    refine_face_landmarks=True) as holistic:\n",
    "\n",
    "    while video.isOpened(): \n",
    "        # Capture frame-by-frame\n",
    "        success, frame = video.read()\n",
    "       \n",
    "        if not success:\n",
    "            print(\"Ignoring empty frame\")\n",
    "            # if loading a video, use break or else use continue for live stream\n",
    "            break\n",
    "        \n",
    "\n",
    "        # Convert the BGR image to RGB before processing.\n",
    "        \n",
    "        results = holistic.process(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
    "        \n",
    "        # Draw pose, left and right hands, and face landmarks on the image.\n",
    "        if results.face_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                results.face_landmarks,\n",
    "                mp_holistic.FACEMESH_TESSELATION,\n",
    "                landmark_drawing_spec=None,\n",
    "                connection_drawing_spec=mp_drawing_styles\n",
    "                .get_default_face_mesh_tesselation_style())\n",
    "        if results.pose_landmarks:\n",
    "            mp_drawing.draw_landmarks(\n",
    "                frame,\n",
    "                results.pose_landmarks,\n",
    "                mp_holistic.POSE_CONNECTIONS,\n",
    "                landmark_drawing_spec=mp_drawing_styles.\n",
    "                get_default_pose_landmarks_style())\n",
    "\n",
    "        # Uncomment following if you want to write to a new video file\n",
    "        output.write(frame)\n",
    "       \n",
    "        # Plot pose world landmarks.\n",
    "        # mp_drawing.plot_landmarks(\n",
    "        #     results.pose_world_landmarks, mp_holistic.POSE_CONNECTIONS)\n",
    "       \n",
    "\n",
    "    # release the video capture object\n",
    "    video.release()\n",
    "\n",
    "    # release the video output object\n",
    "    output.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "949777d72b0d2535278d3dc13498b2535136f6dfe0678499012e853ee9abcab1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
